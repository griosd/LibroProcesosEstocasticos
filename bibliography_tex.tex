\begin{thebibliography}{99}
\bibitem{A} Williams, C. K., \& Rasmussen, C. E. (2006). Gaussian processes
for machine learning. the MIT Press, 2(3), 4.

\bibitem{B} Sch\"{o}lkopf, B., \& Smola, A. J. (2002). Learning with
kernels: support vector machines, regularization, optimization, and beyond.
MIT press.

\bibitem{C} Bishop, C. M. (2006). Pattern Recognition. Machine Learning.

\bibitem{D} Murphy, K. P. (2012). Machine learning: a probabilistic
perspective. MIT press.

\bibitem{E} Barber, D. (2012). Bayesian reasoning and machine learning.
Cambridge University Press.

\bibitem{F} Friedman, A. (1970). Foundations of modern analysis. Courier
Corporation.

\bibitem{1} Williams, C. K., \& Rasmussen, C. E. (1996). Gaussian processes
for regression.

\bibitem{2} Turner, R. D., Deisenroth, M. P., \& Rasmussen, C. E. (2010).
State-space inference and learning with Gaussian processes. In International
Conference on Artificial Intelligence and Statistics (pp. 868-875).

\bibitem{3} Frigola, R., Lindsten, F., Sch\"{o}n, T. B., \& Rasmussen, C.
(2013). Bayesian inference and learning in Gaussian process state-space
models with particle MCMC. In Advances in Neural Information Processing
Systems (pp. 3156-3164).

\bibitem{4} Frigola, R., Chen, Y., \& Rasmussen, C. (2014). Variational
Gaussian process state-space models. In Advances in Neural Information
Processing Systems (pp. 3680-3688).

\bibitem{5} Tobar, F., Djuric, P. M., \& Mandic, D. P. (2015). Unsupervised
State-Space Modeling Using Reproducing Kernels. Signal Processing, IEEE
Transactions on, 63(19), 5210-5221.

\bibitem{6} Steinwart, I., Hush, D., \& Scovel, C. (2006). An explicit
description of the reproducing kernel Hilbert spaces of Gaussian RBF
kernels. Information Theory, IEEE Transactions on, 52(10), 4635-4643.

\bibitem{7} Minh, H. Q. (2010). Some properties of Gaussian reproducing
kernel Hilbert spaces and their implications for function approximation and
learning theory. Constructive Approximation, 32(2), 307-338.

\bibitem{8} Fukumizu, K., Song, L., \& Gretton, A. (2013). Kernel Bayes'
rule: Bayesian inference with positive definite kernels. The Journal of
Machine Learning Research, 14(1), 3753-3783.

\bibitem{9} Smola, A., Gretton, A., Song, L., \& Sch\"{o}lkopf, B. (2007,
October). A Hilbert space embedding for distributions. In Algorithmic
learning theory (pp. 13-31). Springer Berlin Heidelberg.

\bibitem{10} Song, L., Fukumizu, K., \& Gretton, A. (2013). Kernel
embeddings of conditional distributions: A unified kernel framework for
nonparametric inference in graphical models. Signal Processing Magazine,
IEEE, 30(4), 98-111.

\bibitem{11} Kanagawa, M., Nishiyama, Y., Gretton, A., \& Fukumizu, K.
(2014, June). Monte Carlo Filtering Using Kernel Embedding of Distributions.
In AAAI (pp. 1897-1903).

\bibitem{12} Duvenaud, D., Lloyd, J. R., Grosse, R., Tenenbaum, J. B., \&
Ghahramani, Z. (2013). Structure discovery in nonparametric regression
through compositional kernel search. arXiv preprint arXiv:1302.4922.

\bibitem{13} Lloyd, J. R., Duvenaud, D., Grosse, R., Tenenbaum, J. B., \&
Ghahramani, Z. (2014). Automatic construction and natural-language
description of nonparametric regression models. arXiv preprint
arXiv:1402.4304.

\bibitem{14} Wilson, A. G., \& Adams, R. P. (2013). Gaussian process kernels
for pattern discovery and extrapolation. arXiv preprint arXiv:1302.4245.

\bibitem{16} Tobar, F., Bui, T. D., \& Turner, R. E. (2015). Design of
Covariance Functions using Inter-Domain Inducing Variables. In Time Series
Workshop - NIPS 28

\bibitem{17} Tobar, F., Bui, T. D., \& Turner, R. E. (2015). Learning
stationary time series using Gaussian processes with nonparametric kernels.
In Advances in Neural Information Processing Systems (pp. 3483-3491).

\bibitem{18} Ambikasaran, S., Foreman-Mackey, D., Greengard, L., Hogg, D.,
\& O Neil, M. (2016). Fast Direct Methods for Gaussian Processes.

\bibitem{19} Gonzalez, J., \& Hong, S. A. Linear-Time Inverse Covariance
Matrix Estimation in Gaussian Processes.

\bibitem{20} Bui, T. D., \& Turner, R. E. (2014). Tree-structured Gaussian
process approximations. In Advances in Neural Information Processing Systems
(pp. 2213-2221).

\bibitem{21} Snelson, E., \& Ghahramani, Z. (2005). Sparse Gaussian
processes using pseudo-inputs. In Advances in neural information processing
systems (pp. 1257-1264).

\bibitem{22} Quinonero-Candela, J., \& Rasmussen, C. E. (2005). A unifying
view of sparse approximate Gaussian process regression. The Journal of
Machine Learning Research, 6, 1939-1959.

\bibitem{23} Hensman, J., Fusi, N., \& Lawrence, N. D. (2013). Gaussian
processes for big data. arXiv preprint arXiv:1309.6835.

\bibitem{24} Titsias, M. K. (2009). Variational learning of inducing
variables in sparse Gaussian processes. In International Conference on
Artificial Intelligence and Statistics (pp. 567-574).

\bibitem{25} Gal, Y., \& van der Wilk, M. (2014). Variational Inference in
Sparse Gaussian Process Regression and Latent Variable Models-a Gentle
Tutorial. arXiv preprint arXiv:1402.1412.

\bibitem{26} Matthews, A. G. D. G., Hensman, J., Turner, R. E., \&
Ghahramani, Z. (2015). On Sparse variational methods and the
Kullback-Leibler divergence between stochastic processes. arXiv preprint
arXiv:1504.07027.

\bibitem{27} Boyle, P., \& Frean, M. (2004). Dependent gaussian processes.
In Advances in neural information processing systems (pp. 217-224).

\bibitem{28} Seeger, M., Teh, Y. W., \& Jordan, M. (2005). Semiparametric
latent factor models (No. EPFL-REPORT-161465).

\bibitem{29} Titsias, M. K., \& Lawrence, N. D. (2010). Bayesian Gaussian
process latent variable model. In International Conference on Artificial
Intelligence and Statistics (pp. 844-851).

\bibitem{30} Duvenaud, D. K., Nickisch, H., \& Rasmussen, C. E. (2011).
Additive gaussian processes. In Advances in neural information processing
systems (pp. 226-234).

\bibitem{31} Alvarez, M. A., \& Lawrence, N. D. (2011). Computationally
efficient convolved multiple output gaussian processes. The Journal of
Machine Learning Research, 12, 1459-1500.

\bibitem{32} Figueiras-vidal, A., \& L\'{a}zaro-gredilla, M. (2009).
Inter-domain Gaussian processes for sparse inference using inducing
features. In Advances in Neural Information Processing Systems (pp.
1087-1095).

\bibitem{33} Alvarez, M., \& Lawrence, N. D. (2009). Sparse convolved
Gaussian processes for multi-output regression. In Advances in neural
information processing systems (pp. 57-64).

\bibitem{34} Yang, X., \& Maciejowski, J. M. (2015). Fault tolerant control
using Gaussian processes and model predictive control. International Journal
of Applied Mathematics and Computer Science, 25(1), 133-148.

\bibitem{35} Archambeau, C., Cornford, D., Opper, M., \& Shawe-Taylor, J.
(2007). Gaussian process approximations of stochastic differential
equations. Journal of machine learning research, 1, 1-16.

\bibitem{36} Murray, I., MacKay, D., \& Adams, R. P. (2009). The Gaussian
process density sampler. In Advances in Neural Information Processing
Systems (pp. 9-16).

\bibitem{37} Green, P. J. (1995). Reversible jump Markov chain Monte Carlo
computation and Bayesian model determination. Biometrika, 82(4), 711-732.

\bibitem{38} MacKay, D. J. (1999). Comparison of approximate methods for
handling hyperparameters. Neural computation, 11(5), 1035-1068.

\bibitem{39} Powell, M. J. (2007). A view of algorithms for optimization
without derivatives. Mathematics Today-Bulletin of the Institute of
Mathematics and its Applications, 43(5), 170-174.

\bibitem{40} Powell, M. J. (2009). The BOBYQA algorithm for bound
constrained optimization without derivatives. Cambridge NA Report NA2009/06,
University of Cambridge, Cambridge.

\bibitem{41} Gratton, S., Royer, C. W., Vicente, L. N., \& Zhang, Z. (2015).
Direct search based on probabilistic descent. SIAM Journal on Optimization,
25(3), 1515-1541.

\bibitem{42} Tobar, F., \& Turner, R. E. (2015, April). Modelling of complex
signals using gaussian processes. In Acoustics, Speech and Signal Processing
(ICASSP), 2015 IEEE International Conference on (pp. 2209-2213). IEEE.

\bibitem{43} Bonilla, E. V., Chai, K. M., \& Williams, C. (2007). Multi-task
Gaussian process prediction. In Advances in neural information processing
systems (pp. 153-160).

\bibitem{44} Bonilla, E. V., Agakov, F. V., \& Williams, C. (2007). Kernel
multi-task learning using task-specific features. In International
Conference on Artificial Intelligence and Statistics (pp. 43-50).

\bibitem{45} Wilson, A., Ghahramani, Z., \& Knowles, D. A. (2012). Gaussian
Process Regression Networks. In Proceedings of the 29th International
Conference on Machine Learning (ICML-12) (pp. 599-606).

\bibitem{46} Alvarez, M. A., Rosasco, L., \& Lawrence, N. D. (2011). Kernels
for vector-valued functions: A review. arXiv preprint arXiv:1106.6251.

\bibitem{47} Aronszajn, N. (1950). Theory of reproducing kernels.
Transactions of the American mathematical society, 68(3), 337-404.

\bibitem{48} Kimeldorf, G., \& Wahba, G. (1971). Some results on
Tchebycheffian spline functions. Journal of mathematical analysis and
applications, 33(1), 82-95.

\bibitem{49} Gihman, I. I. and Skorohod, A. V. (1974), The theory of
stochastic processes 1 (Springer, Berlin).

\bibitem{50} Byron, M. Y., Cunningham, J. P., Santhanam, G., Ryu, S. I.,
Shenoy, K. V., \& Sahani, M. (2009). Gaussian-process factor analysis for
low-dimensional single-trial analysis of neural population activity. In
Advances in neural information processing systems (pp. 1881-1888).

\bibitem{51} Friedman, N., \& Nachman, I. (2000, June). Gaussian process
networks. In Proceedings of the Sixteenth conference on Uncertainty in
artificial intelligence (pp. 211-219). Morgan Kaufmann Publishers Inc.

\bibitem{52} Damianou, A. C., \& Lawrence, N. D. (2013, August). Deep
Gaussian Processes. In AISTATS (pp. 207-215).

\bibitem{53} Duvenaud, D. K., Rippel, O., Adams, R. P., \& Ghahramani, Z.
(2014, February). Avoiding pathologies in very deep networks. In AISTATS
(pp. 202-210).

\bibitem{54} Chan, A. B., \& Dong, D. (2011, June). Generalized Gaussian
process models. In CVPR (pp. 2681-2688).

\bibitem{55} Neal, R. M. (1996). Priors for infinite networks. In Bayesian
Learning for Neural Networks (pp. 29-53). Springer New York.

\bibitem{56} Wilson, A. G., \& Ghahramani, Z. (2010). Generalised wishart
processes. arXiv preprint arXiv:1101.0240.

\bibitem{57} Goldberg, P. W., Williams, C. K., \& Bishop, C. M. (1997).
Regression with input-dependent noise: A Gaussian process treatment.
Advances in neural information processing systems, 10, 493-499.

\bibitem{58} Wang, J., Hertzmann, A., \& Blei, D. M. (2005). Gaussian
process dynamical models. In Advances in neural information processing
systems (pp. 1441-1448).

\bibitem{59} Kersting, K., Plagemann, C., Pfaff, P., \& Burgard, W. (2007,
June). Most likely heteroscedastic Gaussian process regression. In
Proceedings of the 24th international conference on Machine learning (pp.
393-400). ACM.

\bibitem{60} Adams, R. P., \& Stegle, O. (2008, July). Gaussian process
product models for nonparametric nonstationarity. In Proceedings of the 25th
international conference on Machine learning (pp. 1-8). ACM.

\bibitem{61} Titsias, M. K., \& L\'{a}zaro-gredilla, M. (2011). Variational
heteroscedastic Gaussian process regression. In Proceedings of the 28th
International Conference on Machine Learning (ICML-11) (pp. 841-848).

\bibitem{62} Wilson, A., \& Ghahramani, Z. (2010). Copula processes. In
Advances in Neural Information Processing Systems (pp. 2460-2468).

\bibitem{63} Wu, Y., Hern\'{a}ndez-Lobato, J. M., \& Ghahramani, Z. (2014).
Gaussian process volatility model. In Advances in Neural Information
Processing Systems (pp. 1044-1052).

\bibitem{64} Fox, E., \& Dunson, D. (2011). Bayesian nonparametric
covariance regression. arXiv preprint arXiv:1101.2017.

\bibitem{65} Adams, R. P., Murray, I., \& MacKay, D. J. (2009, June).
Tractable nonparametric Bayesian inference in Poisson processes with
Gaussian process intensities. In Proceedings of the 26th Annual
International Conference on Machine Learning (pp. 9-16). ACM.

\bibitem{66} Shah, A., Wilson, A. G., \& Ghahramani, Z. (2014, February).
Student-t Processes as Alternatives to Gaussian Processes. In AISTATS (pp.
877-885).

\bibitem{67} Snoek, J., Larochelle, H., \& Adams, R. P. (2012). Practical
bayesian optimization of machine learning algorithms. In Advances in neural
information processing systems (pp. 2951-2959).

\bibitem{68} Martinez-Cantin, R. (2014). BayesOpt: a Bayesian optimization
library for nonlinear optimization, experimental design and bandits. Journal
of Machine Learning Research, 15(1), 3735-3739.


\bibitem{MMM-MarketShare} {\small Cavender, David et al. "Automated
	specification, estimation, discovery of causal drivers and market response
	elasticities or lift factors." US Patent 8468045 B2 (2013).}

\bibitem{MMM-Tellis} {\small Tellis, Gerard J. "Modeling marketing mix."
	Handbook of marketing research (2006): 506-522.}

\bibitem{DBN-BookN} {\small Neapolitan, Richard E. Learning bayesian
	networks. Vol. 38. Upper Saddle River: Prentice Hall, 2004.}

\bibitem{DBN-StateOfArt} {\small Mihajlovic, V., and Milan Petkovic.
	"Dynamic bayesian networks: A state of the art." (2001).}

\bibitem{DBN-LearningDBN} {\small Ghahramani, Zoubin. "Learning dynamic
	Bayesian networks." Adaptive processing of sequences and data structures.
	Springer Berlin Heidelberg (1998). 168-197.}

\bibitem{DBN-Murphy} {\small Murphy, Kevin Patrick. Dynamic bayesian
	networks: representation, inference and learning. Diss. University of
	California, Berkeley (2002).}

\bibitem{DBN-DAEM} {\small Ueda, Naonori, and Ryohei Nakano. "Deterministic
	annealing EM algorithm." Neural Networks 11.2 (1998): 271-282.}
\end{thebibliography}