%!TEX root = main.tex
\chapter{Introducción}

\begin{chapquote}{Arthur Samuel, 1959}
	``El aprendizaje de máquinas es el campo de estudio que le da a las computadoras la capacidad de aprender sin ser programadas explícitamente.''
\end{chapquote}

\comment{Definir bien el publico objetivo: deberia ser tanto estudiasten con conocimiento de machine learning, como profesores buscando un libro para su estanteria, tambien profesionales y estudiantes de doc.}


\comment{partir hablanco para quien es este libro, que es lo que tiene}
Debido al tremendo desarrollo tecnológico de la última década, la cantidad de datos generados y recopilados ha alcanzado dimensiones desconocidas para la humanidad hasta ahora. Por este hecho, el campo del aprendizaje de máquinas, o automático, ha llamado la atención de manera transversal en diversas áreas de las ciencias naturales, las ciencias sociales, la ingeniería y la medicina, por mencionar algunas. Además, el desarrollo no solo se ha dado en la academia, sino que grandes empresas como Google, Facebook y Amazon han fortalecido el área con sus propios equipos de investigación y desarrollo. 

La sociedad ha vivido tres revoluciones industriales: la revolución de la máquina de vapor y la imprenta (1760--1840), la revolución de la electricidad y el motor a petróleo (1870--1900), y la tercera revolución industrial de internet, información y comunicación (1969 a la fecha). La sociedad vive la cuarta revolución industrial, la de la inteligencia artificial y los datos, donde el aprendizaje automático ocupa un papel central: hacer que las máquinas aprendan de los datos.

Existen múltiples clasificaciones y divisiones de los métodos y modelos de aprendizaje automático, pero podemos destacar una característica que los divide en dos grandes grupos: los que tienen un enfoque probabilístico y los que no. Este libro adopta el punto de vista de que la mejor manera de hacer máquinas que puedan aprender de los datos es a través de las herramientas de la teoría de probabilidad, el pilar de la estadística y la ingeniería durante siglos. Podemos decir, con orgullo, que este libro sigue el paradigma bayesiano. Es un hecho que la mayoría de los datos del mundo real son observaciones ruidosas de fenómenos latentes, por lo que es necesario modelar estas fuentes aleatorias, y el paradigma bayesiano nos permite inferir modelos y hacer predicciones de forma natural.

\comment{explicar un poco más el concepto}
Después de una introducción al paradigma bayesiano para el modelado, este trabajo introduce los procesos gaussianos (GP por sus siglas en inglés): distribuciones \emph{a priori} no paramétricas sobre funciones, utilizadas como modelos generativos con propiedades de modelado atractivas para la inferencia bayesiana. Pueden modelar relaciones no lineales con observaciones ruidosas, tienen expresiones con forma cerrada para el entrenamiento e inferencia, y se rigen por hiperparámetros interpretables.

En este trabajo se realiza una revisión del estado del arte de algunos de los aspectos más importantes de los modelos basados en GP, los cuales presentan un poderoso esquema de estimación de funciones aplicado principalmente a los problemas de regresión. Una vez que entendemos su elegancia como modelos de regresión, esto nos hace preguntarnos: ¿existen procesos estocásticos más generales que mantengan la misma gracia y belleza que los procesos gaussianos? Los GP se basan en la gaussianidad, una suposición que no es cierta en varios escenarios del mundo real. Por ejemplo, observaciones acotadas por ciertos límites, o con dependencias de valor extremo (fenómenos naturales en física, finanzas y ciencias sociales) no siguen la suposición de gaussianidad.

Para modelar datos no gaussianos, es útil considerar el \emph{compositionally-warped} GP, un modelo generativo no gaussiano computacionalmente eficiente. Después se estudia el concepto de cópula de procesos estocásticos para así introducir los modelos basados en procesos elípticos. Componiendo estos elementos es posible aislar dependencia de la correlación y de sus marginales, por lo que el modelo propuesto, llamado proceso de transporte gaussiano (TGP por sus siglas en inglés), abarca los modelos de GP, \emph{warped} GP, procesos de \(t\) de Student, y otros modelos bajo un enfoque unificador. Para todos los casos proporcionamos expresiones analíticas y algoritmos para el entrenamiento e inferencia de los modelos de regresión.

\comment{extender harto esto, la idea es hacer un mapa de como leer el libro}
Este libro sigue un orden basados en capítulos autocontenidos, en el sentido que se abarca en profundidad un tema particular de los procesos gaussianos que es necesario para hacer modelos más expresivos. El libro se distribuye en los siguientes capítulos:

\comment{tratar de evitar lenguaje muuuy tecnico}
\begin{enumerate}
	\item \textbf{Marco Teórico}: se definen los conceptos básicos de probabilidades, como los tipos de distribuciones de probabilidad, las reglas de marginalización y condicionalidad, el teorema de Bayes y el teorema de consistencia de Kolmogorov, que son necesarios para definir y entender mejor que son los procesos estocásticos y sus propiedades básicas, para luego introducir la familia particular y distinguida de los procesos gaussianos.
	
	\item \textbf{Modelos Bayesianos de Regresión}: se introduce el paradigma bayesiano para modelos de regresión, el enfoque paramétrico, los estimadores de Bayes, y modelos bayesianos jerárquicos. Luego se muestran los diferentes métodos de inferencia bayesiana utilizados en la literatura para el entrenamiento de modelos, tales como muestreo por rechazo o por importancia, o los métodos basados en las cadenas de Markov de Monte Carlo (MCMC en inglés).
	
	\item \textbf{Procesos Gaussianos de Regresión}: se realiza una construcción desde los modelos de regresión lineal y no lineal, incorporando ruido a la señal observada, estimación bayesiana de los parámetros, versiones no paramétricas de los modelos, para luego introducir la función de kernel, que juego un papel fundamental en la definición de un proceso gaussiano. La función de kernel puede ser interpretada como un producto punto de un espacio de gran dimensión o como la función de covarianza de un proceso gaussiano, que corresponde a una distribución de probabilidades de un espacio de funciones. Otro elemento importante es la incorporación de una función de media, que puede modelarse de forma bayesiana agregando mayor expresividad al modelo. Finalmente se profundiza en la tarea de entrenamiento del modelo, analizando el enfoque bayesiano y el enfoque de máxima verosimilitud, incorporando una función de riesgo para la selección de los parámetros, versión modificada de la verosimilitud si se considera realizar validación cruzada, y las versiones de los kernels si se desea realizar selección de variables.
	
	\item \textbf{Funciones de Kernel}: se revisan resultados clásicos de análisis funcional sobre kernels, los cuales nos permiten relacionar la propiedad de «definición positiva» con sus valores propios, definir el espacio de funciones que reproduce un kernel y caracterizarlo como un espacio de Hilbert, aplicando teoremas de representación para caracterizar sus elementos. Después se realiza una caracterización de algunos de los kernels estacionarios más importantes, y se revisan kernels no estacionarios tales como los de producto punto. Finalmente se ven las diferentes operaciones que podemos realizar sobre uno o más kernels para obtener nuevos kernels que mantengan la propiedad de ser definido positivo, estudiando los diferentes efectos obtenidos dependiendo de los kernels a componer, para terminar analizando como se componen y como se comportan en el caso que el espacio de entrada sea multidimensional.
	
	\item \textbf{Procesos Gaussianos \textit{Sparse}}: se presentan las versiones \emph{sparse} de los procesos gaussianos que se utilizan para reducir la complejidad computacional de los modelos, ya que la inversión de la matriz de covarianza es muy costosa computacionalmente, llegando a no ser tratable en el caso que la cantidad de datos sea grande. Se presenta un enfoque que unifica las diferentes aproximaciones \emph{sparse} bajo los supuestos de independencia condicional entre los datos y las variables latentes llamadas \emph{pseudo-inputs}, las cuales se utilizan para aproximar los datos originales de la mejor forma posible. Los modelos presentados se diferencian entre sí en la forma de modelar dichas dependencias, produciendo en algunos casos modelos que no son procesos gaussianos, pero bajo supuestos razonables es posible de mantener la propiedad de gaussianidad. Estos métodos permiten bajar la complejidad computacional de \( O\left(n^{3}\right) \) a \( O\left(m^{2} n\right) \), donde \(n\) es la cantidad de datos y \(m\) es la cantidad de \emph{pseudo-inputs}.
	
	\item \textbf{Procesos Gaussianos Multioutput}: se introducen los procesos gaussianos con salida multidimensional, donde se presentan los principales modelos que extienden a los procesos gaussianos unidimensionales a varias dimensiones. Mientras que la forma más directa de modelar múltiples salidas es utilizando varios procesos independientes en paralelo, este enfoque no incorpora la información que pueden entregar las diferentes salidas entre si. El enfoque clásico se basa en aumentar el espacio de entrada del kernel en una dimensión categórica, que indica cuál es la salida, y componer esta dimensión de forma multiplicativa se basa en multiplicar el kernel por un único coeficiente por cada par de salidas. Un enfoque novedoso se deriva de modelar el espacio de salida como el plano complejo, por lo que el proceso debe cumplir las propiedades de una normal compleja, que se modela con las funciones de covarianza y de pseudocovarianza, para luego derivar el proceso real bidimensional equivalente. Una forma de abarcar varias dimensiones es considerar varios procesos gaussianos latentes, y las salidas se modelan como combinaciones lineales de dichos procesos. Mientras todos estos modelos se basan en funciones de covarianza auxiliares, una forma más general es considerar filtros lineales de convolución, los cuales son más flexibles y permiten modelar funciones de covarianza cruzada más expresivas.
	
	\item \textbf{Procesos Gaussianos Deformados}: así como algunos modelos transforman los datos de entrada, el modelo de Procesos Gaussianos Deformados (WGP por sus siglas en inglés) aplica una transformación no lineal a los datos de salida, y aplicando el teorema de cambio de variables somos capaces de calcular la log-verosimilitud de los hiperparámetros y la distribución posterior del proceso. Se describe una familia de transformaciones elementales no lineales, que se pueden componer para generar un espacio de transformaciones más expresivo que el estado del arte.
	
	\item \textbf{Procesos Gaussianos Transportados}: en este capítulo se muestra que el modelo WGP tiene marginales no gaussianas, pero introduciendo el concepto de cópula se demuestra que la estructura de dependencia es intrínsecamente gaussiana. A continuación se introducen los transportes radiales para considerar el caso de la \(t\) de Student y finalizar con la generalización de los GP y WGP, llamados Procesos Gaussianos Transportados (TGP en inglés).
	
	\item \textbf{Marketing Mix Modeling}: en este último capítulo, se enseña a construir modelos de marketing con diferentes variables de control, como la inversión de medios, el precio y la distribución, para estimar la variable objetiva de ventas. La construcción del modelo se realiza a través de los Procesos Gaussianos Transportados presentados anteriormente. Se explica como incorporar efectos paramétricos aditivos y multiplicativos, así como la existencia de una componente no paramétrica en el modelo. Esta última representa la suma de todos los efectos no modelados directamente, tales como la competencia y la economía. Después, se analiza el papel que juega el paradigma bayesiano en este tipo de modelos, mostrando como llevar a cabo el entrenamiento y la simulación, así como la forma de visualizar los resultados. Finalmente se muestra cómo se utiliza este tipo de modelos para optimizar la inversión en medios de forma eficiente.
	
	\item \textbf{Anexo de Optimización}: en este anexo se encuentra una rápida introducción a la optimización no lineal, explicando diferencias entre las derivadas numéricas y derivación automática, presentando diferentes métodos iterativos de optimización, criterios de parada, reglas del tamaño del paso, planteamiento del problema de control óptimo, restricciones de igual, desigualdad e intervalo, y se muestra el método de multiplicadores de Lagrange para resolver esos problemas.
\end{enumerate}

